---
title: "inc_quant_toilet_paper"
author: "Lieve Heyrman"
date: '2024-05-13'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r set up file}

rm(list=ls())

library(here)

# set project_path
project_path <- here::here()

# set path to data files (depending on operating computer)
location <- "local"
if (location == "server") {
  if (Sys.info()["nodename"] == "GHUM-L-E939DCHK"){
    data_path <- "X:/Retailing_Group_RawData/Hoarding data"
  }else if (Sys.info()["nodename"] == "GHUM-L-E0376K0Q") {
    data_path <- "X:/Retailing_Group_RawData/Hoarding data" }
} else if (location == "local") { 
  if (Sys.info()["nodename"] == "GHUM-L-E939DCHK"){
    data_path <- ""
  }else if (Sys.info()["nodename"] == "GHUM-L-E0376K0Q") {
    data_path <- "C:/PhD KU Leuven/AiMark" } 
}

# load libraries
list.of.packages <- c("plyr","tidyr","ggplot2", "tidyverse", "zoo", "readxl", "car", "sampleSelection", "stargazer", "haven","expss", "plm", "sandwich", "lmtest")
# Install package if they are not installed yet
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Load packages
lapply(list.of.packages, require, character.only = TRUE)

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)

options(scipen = 999)

# create country list
country_list <- c("NL","BE","GE","FR","UK")
# set country
country <- country_list[country_list=="NL"]

```

To avoid unnecessary running time: break here.
The prepared dataset has been saved, and can be directly loaded in the code below.

``` {r break}

break()

```





# CREATING FULL DATA SET

1. Loading the prepared Price Index and Promo Index

``` {r loading Price and Promo Indices}

load(paste(project_path,"/INC-QUANT model/toilet paper/PricePromoIndex_NL.RData",sep=""))

# rename for clarity
PricePromoIndex <- final_PricePromoIndex
rm(final_PricePromoIndex)
```


2. Loading and preparing the purchase data

```{r loading purchase data}

# loading the filtered purchase data
load(paste(project_path,"/INC-QUANT model/toilet paper/prepared_purchasedata_NL_final",sep=""))

# get a list of all dates, household IDs and brand-sub-brand-retailers
all.dates <- unique(TP_purch$Date_of_purchase)
all.households <- unique(TP_purch$Panelist)
all.BSBRetailers <- unique(TP_purch$BSBRetailer)
# get a list of all estimation dates (= excluding the initialization period, i.e. 2018)
estimation.dates <- seq.Date(from=as.Date("2019-01-01"), to=as.Date("2020-04-30"), by="days")

# for each household, we no longer make a distinction between different products; just whether they bought and if so, how many rolls
TP_purch <- ddply(TP_purch, .(Date_of_purchase, Panelist), summarise, Total_volume_sales = sum(Total_volume_sales))

# based on moderators, 243 households excluded -> exclude also from PricePromoIndex dataframe, as this is basis for FullData
PricePromoIndex <- PricePromoIndex[PricePromoIndex$Household %in% all.households,]

```


3. Creating the final dataset 

```{r creating final dataset}

### SETTING UP THE DATAFRAME

# dimensions: #dates X #households
# for every date, for every households: do they buy, and if yes, how much
FullData <- PricePromoIndex
rm(PricePromoIndex)



### ADDING THE PURCHASES 

FullData <- merge(FullData, TP_purch, by.x=c("Date","Household"), by.y=c("Date_of_purchase","Panelist"), all.x=T)
rm(TP_purch)
# purchase flag: yes or no
FullData$PurchaseFlag <- ifelse(!is.na(FullData$Total_volume_sales), 1, 0)



### RESTRICTING FULLDATA TO THE ESTIMATION PERIOD 
FullData <- FullData[FullData$Date %in% estimation.dates,]



### ADDING TIME DUMMIES
library(lubridate)
# day of the week dummies
FullData$Weekday <- wday(FullData$Date, week_start = 1)
FullData$Weekday <- factor(FullData$Weekday)
# week of the year dummies
FullData$Week <- week(FullData$Date)
FullData$Week <- factor(FullData$Week)
# month of the year dummies
FullData$Month <- month(FullData$Date)
FullData$Month <- factor(FullData$Month)
# quarter of the year dummies
FullData$Quarter <- quarter(FullData$Date)
FullData$Quarter <- factor(FullData$Quarter)
# year
FullData$Year <- year(FullData$Date)
FullData$Year <- factor(FullData$Year)



### ADDING THE ESTIMATED INVENTORY
load(paste(project_path,"/Preparation/toilet paper inventory/",country,"_INV_toiletpaper.RData",sep=""))
FullData <- merge(FullData, Inv, by.x=c("Date","Household"), by.y=c("Date", "ID"))
rm(Inv)
# mean centering inventory with household mean of 2019 and 2020
FullData <- FullData %>% group_by(Household) %>% mutate(meanINV = mean(Inv))
FullData$HHInv <- FullData$Inv - FullData$meanINV
col_remove <- c("meanINV")
FullData <- FullData[,!colnames(FullData) %in% col_remove]
rm(col_remove)


### CALCULATING RESTOCKING INVENTORY
# when consumers buy toilet paper, how many rolls are in their inventory at that time?
# we calculate this in 2019
Restock_Inv <- FullData %>% group_by(Household) %>% filter(PurchaseFlag == 1) %>% filter(Date %in% seq.Date(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by="days"))
Restock_Inv <- Restock_Inv %>% group_by(Household) %>% summarize(RestockInventory_nonMC = mean(Inv))
summary(Restock_Inv$RestockInventory_nonMC)
# mean centering across households
Restock_Inv$meanRestock_Inv <- mean(Restock_Inv$RestockInventory_nonMC)
Restock_Inv$RestockInventory_MC <- Restock_Inv$RestockInventory_nonMC - Restock_Inv$meanRestock_Inv
# add to FullData
FullData <- merge(FullData, Restock_Inv, by="Household", all.x=T)
rm(Restock_Inv)



## ADDING AVERAGE INVENTORY IN 2019
AvgInv <- FullData %>% group_by(Household) %>% filter(Date %in% seq.Date(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by="days")) %>% summarize(AvgInventoryLevel_nonMC = mean(Inv))
# grand mean centering
AvgInv$meanAvgInv <- mean(AvgInv$AvgInventoryLevel_nonMC)
AvgInv$AvgInventoryLevel <- AvgInv$AvgInventoryLevel_nonMC - AvgInv$meanAvgInv
# add to FullData
FullData <- merge(FullData, AvgInv[,c("AvgInventoryLevel_nonMC", "AvgInventoryLevel", "Household")], by="Household", all.x=T)
rm(AvgInv)



### ADDING THE PANIC INDEX
panic <- data.frame(Date = estimation.dates)
temp <- read_dta(file="C:/PhD KU Leuven/OneDrive - KU Leuven/Projects_Personal/Project 1 - Stockpiling vs Panic Buying/Panic_COVID19_Data/Panic_Index_Data.dta")
temp <- temp[temp$geoCode == "NL",]
temp$Date <- as.Date(temp$time)
panic <- merge(panic, temp, by="Date", all.x=T)
panic <- panic[, c("Date", "panic_index")]
rm(temp)
# NEW VERSION: impute 2019 and scale between 0 and 1 (like PromoIndex)
# calculate mean panic index of January 2020
panic_jan2020 <- mean(panic$panic_index[panic$Date %in% seq.Date(from=as.Date("2020-01-01"), to=as.Date("2020-01-31"), by="days")])
# for 2019 (no panic index available): impute mean panic index of January 2020
panic$panic_index[is.na(panic$panic_index)] <- panic_jan2020
rm(panic_jan2020)
# rescale panic index to range between 0 and 1
min_panic <- min(panic$panic_index)
max_panic <- max(panic$panic_index)
panic$panic_index_rescaled <- (panic$panic_index - min_panic) / (max_panic - min_panic)
summary(panic$panic_index_rescaled)
# add to FullData
FullData <- merge(FullData, panic, by="Date", all = T)
FullData$PanicIndex <- FullData$panic_index_rescaled
rm(list=c('panic','min_panic','max_panic'))
col_remove <- c("panic_index", "panic_index_rescaled")
FullData <- FullData[,!colnames(FullData) %in% col_remove]
rm(col_remove)



### ADDING THE PREPARED MODERATORS
load(paste(project_path,"/INC-QUANT model/toilet paper/prepared_moderators_NL",sep=""))
FullData <- merge(FullData, moderators, by.x="Household", by.y="Panelist", all.x=T)
rm(moderators)
col_remove <- c("TP_volume", "Promo_value","Promo_volume","PL_value","PL_volume")
FullData <- FullData[,!colnames(FullData) %in% col_remove]
rm(col_remove)

# NOTE: the exclusion restrictions are already prepared as part of the moderator "Familiarity"
   # incidence model: grand mean-centered average interpurchase time of household in 2018 (in days)
   # quantity model: grand mean-centered average purchase quantity of household in 2018 (in rolls)



### SAVING THE FULL DATA

# to speed up estimating different versions of the Heckamn selection model
save(FullData, file=paste(project_path,"/INC-QUANT model/toilet paper/FullData_with_moderators_new.RData",sep=""))

```


4. Renaming the variables for interpretation, and adding labels

``` {r renaming variables}


### RENAME THE VARIABLES FOR CLEAR INTERPRETATION ###
colnames(FullData)[which(names(FullData) == "ShelfPriceIndex")] <- "PriceIndex"
colnames(FullData)[which(names(FullData) == "HHInv")] <- "HouseholdInventory"
    colnames(FullData)[which(names(FullData) == "Inv")] <- "HouseholdInventory_nonMC"
colnames(FullData)[which(names(FullData) == "RestockInventory_MC")] <- "RestockInventory"
colnames(FullData)[which(names(FullData) == "AvgIPT_MC")] <- "Average_IPT"
    colnames(FullData)[which(names(FullData) == "AvgIPT")] <- "Average_IPT_nonMC"
colnames(FullData)[which(names(FullData) == "AvgQ_MC")] <- "Average_Q"
    colnames(FullData)[which(names(FullData) == "AvgQ")] <- "Average_Q_nonMC"
colnames(FullData)[which(names(FullData) == "Promo.sens_volume_MC")] <- "PromoSensitivity"    
    colnames(FullData)[which(names(FullData) == "Promo.sens_volume")] <- "PromoSensitivity_nonMC"
colnames(FullData)[which(names(FullData) == "PL.share_volume_MC")] <- "PLShare"
    colnames(FullData)[which(names(FullData) == "PL.share_volume")] <- "PLShare_nonMC"
colnames(FullData)[which(names(FullData) == "BrandSupport_MC")] <- "BrandLoyalty"
    colnames(FullData)[which(names(FullData) == "BrandSupport")] <- "BrandLoyalty_nonMC"    
colnames(FullData)[which(names(FullData) == "Income_above_median")] <- "Income"
colnames(FullData)[which(names(FullData) == "Weekday")] <- "Day_of_week"
colnames(FullData)[which(names(FullData) == "Household_size")] <- "HouseholdSize"


### USE LABELS TO CLARIFY OPERATIONALIZATION ###
FullData <- apply_labels(FullData, 
                         PriceIndex = "Weighted price index, based on shelf prices", 
                         RegularPriceIndex = "Weighted price index, based on (derived) regular prices (95th percentile of year-store)",
                         RegularPriceIndex_Q = "Weighted price index, based on (derived) regular prices (95th percentile of quarter-store)", 
                         Total_volume_sales = "Number of toilet paper rolls bought",
                         PurchaseFlag = "1 if purchase was made, 0 otherwise",
                         HouseholdInventory_nonMC = "Daily household inventory, with rolling average daily consumption rate (see prepare_INV_toilet_paper script)",
                         HouseholdInventory = "Daily household inventory, mean-centered within household (with mean of 2019-2020), with rolling average daily consumption rate (see prepare_INV_toilet_paper script)",
                         RestockInventory_nonMC = "Average of inventory household holds at the time of purchase, calculated in 2019",
                         RestockInventory = "Grand mean-centered (across households); Average of inventory household holds at the time of purchase, calculated in 2019",
                         AvgInventoryLevel = "Grand mean-centered (across household); Average household inventory, calculated in 2019",
                         AvgInventoryLevel_nonMC = "Average household inventory, calculated in 2019",
                         Total_budget = "Household's total spending in 2018 across all categories",
                         Total_budget_MC = "Grand mean-centered (across households); Household's total spending in 2018 across all categories",
                         Age = "Effect coded; 1 if household is head of household is older than 55, -1 otherwise",
                         HouseholdSize = "Effect coded, 1 if household has three or more members, -1 otherwise",
                         Income = "Effect coded; 1 if household's annual income is above NL median, -1 otherwise", 
                         Income_dev1 = "Effect coded; 2/3 if household's annual income is in lowest 10th percentile, -1/3 otherwise",
                         Income_dev2 = "Effect coded; 2/3 if household's annual income is in highest 10th percentile, -1/3 otherwise",
                         Income_diff1 = "Effect coded; -1 if household's annual income is in lowest 10th percentile, 0 if in highest 10th percentile, 1 otherwise",
                         Income_diff2 = "Effect coded, -1/2 if household's annual income is in lowest 10th percentile, 1 if in highest 10th percentile, -1/2 otherwise",
                         TP_budget = "Household's total toilet paper spending in 2018",
                         TP_budget_MC = "Grand mean-centered (across households); household's total toilet paper spending in 2018",
                         SoB = "share of Total Budget in 2018 spent on toilet paper",
                         SoB_MC = "Grand mean-centered (across households) ; share of Total Budget in 2018 spent on toilet paper",
                         TP_trips = "Number of shopping trips where toilet paper was bought, in 2018", 
                         Promo_trips =  "Number of shopping trips where (at least one roll of) toilet paper was bought in promotion, in 2018",
                         PL_trips =  "Number of shopping trips where (at least one roll of) Private Label toilet paper was bought, in 2018",
                         PromoSensitivity = "Grand mean-centered (across households); fraction of toilet paper rolls bought in promo (out of total number of rolls bought in 2018)", 
                         PromoSensitivity_nonMC = "Fraction of toilet paper rolls bought in promo (out of total number of rolls bought in 2018)",
                         Promo.sens_value = "Fraction of toilet paper budget spent on promo (out of total budget spent on toilet paper in 2018)",
                         Promo.sens_value_MC = "Grand mean-centered (across households); fraction of toilet paper budget spent on promo (out of total budget spent on toilet paper in 2018)",
                         Promo.sens_trip = "Fraction of toilet paper trips where (at least one roll of) toilet paper was bought in promo (out of all toilet paper trips in 2018)",
                         Promo.sens_trip_MC = "Grand mean-centered (across households) ; Fraction of toilet paper trips where (at least one roll of) toilet paper was bought in promo (out of all toilet paper trips in 2018)",
                         PLShare = "Grand mean-centered (across households); fraction of Private Label toilet paper rolls (out of total number of rolls bought in 2018)",
                         PLShare_nonMC = "Fraction of Private Label toilet paper rolls (out of total number of rolls bought in 2018)",
                         PL.share_value = "Fraction of toilet paper budget spent on Private Label (out of total budget spent on toilet paper in 2018)",
                         PL.share_value_MC = "Grand mean-centered (across households) ; fraction of toilet paper budget spent on Private Label (out of total budget spent on toilet paper in 2018)",
                         PL.share_trip = "Fraction of toilet paper trips where (at least one roll of) Private Label toilet paper was bought (out of all toilet paper trips in 2018)",
                         PL.share_trip_MC = "Grand mean-centered (across households) ; fraction of toilet paper trips where (at least one roll of) Private Label toilet paper was bought (out of all toilet paper trips in 2018)",
                         BrandLoyalty = "Grand mean-centered (across households); brand support measure of Knox & Walker (2001) calculated in 2018",
                         BrandLoyalty_nonMC = "Brand support measure of Knox & Walker (2001) calculated in 2018",
                         BrandSupport_std = "Rescaled BrandLoyalty variable, between 0 and 1",
                         BrandSupport_std_MC = "Grand mean-centered (across households) ; rescaled BrandLoyalty variable, between 0 and 1",
                         Average_IPT = "Grand mean-centered (across households) ; average number of days between two purchases in 2018", 
                         Average_IPT_nonMC = "Average number of days between two purchases in 2018",
                         Average_Q = "Grand mean-centered (across households) ; average number of toilet paper rolls bought during a toilet paper purchase in 2018", 
                         Average_Q_nonMC = "Average number of toilet paper rolls bought during a toilet paper purchase in 2018",
                         AvgInv = "Grand mean-centered (across households) ; average toilet paper inventory (in rolls) during 2019 (because 2018 initializes inventory model)", 
                         AvgInv_MC = "Average toilet paper inventory (in rolls) during 2019 (because 2018 initializes inventory model)"
                         )

# save new version of dataframe, with labels 
save(FullData, file=paste(project_path,"/INC-QUANT model/toilet paper/FullData_with_moderators_and_labels_new.RData",sep=""))
```






# RUNNING THE DIFFERENT MODELS

1. Loading the prepared data

```{r loading the prepared dataset}

load(paste(project_path,"/INC-QUANT model/toilet paper/FullData_with_moderators_and_labels_new.RData",sep=""))
```


For storage purposes, we remove all variables (or operationalizations) that are not in our final model

```{r removing redundant variables}

# final selection of relevant variables
variables.keep <- c("Total_volume_sales",                                         # DV
                    "PromoIndex", "PanicIndex", "HouseholdInventory",             # main variables under investigation
                    "PromoSensitivity", "PLShare","BrandLoyalty",                 # gain-seeking moderators
                    "RestockInventory", "AvgInventoryLevel", "Average_IPT", "Average_Q",     # loss-avoidance moderators
                    "PriceIndex", "Age", "HouseholdSize", "Income",               # controls
                    "Day_of_week", "Week", "Year",
                    "Household", "Date")                                          # needed for clustering of st. err.
FullData <- FullData[ , names(FullData) %in% variables.keep]
```


For our second set of analyses, we need the unconditional purchase quantity.
When a purchase was made, this is equal to the volume sales. If no purchase was made, this is set to zero (instead of NA)

```{r preparing unconditional purchase quantity}

FullData$unconditional_Total_volume_sales <- FullData$Total_volume_sales
FullData$unconditional_Total_volume_sales[is.na(FullData$unconditional_Total_volume_sales)] <- 0

```



2. Checking heterogeneity concerns

```{r Variance Inflation Factors}

# full model, without interactions
test_to_get_VIFs_tobit_selection_full <- glm(Total_volume_sales ~ PromoIndex + PanicIndex + HouseholdInventory + PriceIndex + PromoSensitivity + PLShare + BrandLoyalty + AvgInventoryLevel + Average_IPT + Age + HouseholdSize + Income + Day_of_week + Week + Year, data = FullData)
vif(test_to_get_VIFs_tobit_selection_full)

test_to_get_VIFs_tobit_outcome_full <- glm(Total_volume_sales ~ PromoIndex + PanicIndex + HouseholdInventory + PriceIndex + PromoSensitivity + PLShare + BrandLoyalty + AvgInventoryLevel + Average_Q + Age + HouseholdSize + Income + Day_of_week + Week + Year, data = FullData)
vif(test_to_get_VIFs_tobit_outcome_full)

# full model, with interactions
test_to_get_VIFs_tobit_selection_full_int <- glm(Total_volume_sales ~ PromoIndex + PanicIndex + HouseholdInventory + PriceIndex + (PromoSensitivity + PLShare + BrandLoyalty + AvgInventoryLevel + Average_IPT + Age + HouseholdSize + Income)*PromoIndex + (PromoSensitivity + PLShare + BrandLoyalty + AvgInventoryLevel + Average_IPT + Age + HouseholdSize + Income)*PanicIndex + Day_of_week + Week + Year, data = FullData)
vif(test_to_get_VIFs_tobit_selection_full_int)

test_to_get_VIFs_tobit_outcome_full_int <- glm(Total_volume_sales ~ PromoIndex + PanicIndex + HouseholdInventory + PriceIndex + (PromoSensitivity + PLShare + BrandLoyalty + AvgInventoryLevel + Average_Q + Age + HouseholdSize + Income)*PromoIndex + (PromoSensitivity + PLShare + BrandLoyalty + AvgInventoryLevel + Average_Q + Age + HouseholdSize + Income)*PanicIndex + Day_of_week + Week + Year, data = FullData)
vif(test_to_get_VIFs_tobit_outcome_full_int)



test_to_get_VIFs_ols <- glm(Total_volume_sales ~ PromoIndex + PanicIndex + PromoSensitivity + PLShare + BrandLoyalty + HouseholdInventory + Average_IPT + Average_Q + PriceIndex + Age + HouseholdSize + Income + Day_of_week + Week + Year, data = FullData)
vif(test_to_get_VIFs_ols)

rm(test_to_get_VIFs_tobit_selection_full, test_to_get_VIFs_tobit_selection_full_int, test_to_get_VIFs_tobit_outcome_full, test_to_get_VIFs_tobit_outcome_full_int, test_to_get_VIFs_ols)
```

``` {r correlation matrix}

correlation_matrix <- round(cor(FullData[,c("PanicIndex","PromoIndex","PriceIndex", "HouseholdInventory", "Average_IPT", "Average_Q", "AvgInventoryLevel", "PromoSensitivity", "PLShare","BrandLoyalty", "Age", "HouseholdSize", "Income")]),5)
max(correlation_matrix[correlation_matrix < 1])

# saving the correlation matrix
#stargazer(correlation_matrix, out = paste(project_path,"/R_Files/INC-QUANT model/toilet paper/Tables/correlation_matrix_July20.html", sep=""))

```




3. Estimating the Purchase-Incidence Model

```{r TOBIT 2}

library(modelsummary)
# main effects only, including consumer characteristics as controls (not interacted)
heckman_main <- selection(selection = !is.na(Total_volume_sales) ~  PromoIndex + PanicIndex 
                                      + HouseholdInventory + PriceIndex
                                      + Average_IPT + PromoSensitivity + PLShare + BrandLoyalty
                                      + Age + HouseholdSize + Income + Day_of_week + Week + Year,
                          outcome =          Total_volume_sales  ~  PromoIndex + PanicIndex 
                                      + HouseholdInventory + PriceIndex
                                      + Average_Q + PromoSensitivity + PLShare + BrandLoyalty
                                      + Age + HouseholdSize + Income + Day_of_week + Week + Year,
                          data = FullData, method ="2step")
modelsummary(heckman_main, shape = term ~ component + statistic, stars = T, statistic = c("std.error", "p.value"))

rm(heckman_main)
```


Replicating the findings with the fixed effects model (DV = unconditional quantity)

```{r fixed-effects replication}
# estimate fixed effects model with main effects only, including controls
fe_main <- plm(unconditional_Total_volume_sales ~  PromoIndex + PanicIndex + 
                    HouseholdInventory + Average_IPT + Average_Q + 
                    PriceIndex + Age + HouseholdSize + Income + Day_of_week + Week + Year,
                  data = FullData, index= c("Household", "Date"), model = "within")
# cluster standard errors by household
fe_main_clus <- coeftest(fe_main, vcov = vcovHC, type = "HC1", cluster= "group")

# save output
#fixed_effects_output(fe_main_clus, "FinalModel_replicateStudy1_clustFE_controls_Full")


# clean version: excluding time fixed effects from output
modelmatrix <- model.matrix(fe_main)
keep_main <- colnames(modelmatrix)
rm(modelmatrix)
keep_main <- keep_main[!grepl("Day",keep_main) & !grepl("Week",keep_main) & !grepl("Year",keep_main)]

# save clean output
#fixed_effects_output_clean(fe_main_clus, "FinalModel_replicateStudy1_clustFE_controls_Clean", keep_main)
```



Replicating the findings with the linear regression model (DV = unconditional quantity)

```{r linear regression replication}

# estimate fixed effects model with main effects only, including controls
ols <- lm(unconditional_Total_volume_sales ~  PromoIndex + PanicIndex + 
                 HouseholdInventory + Average_IPT + Average_Q + 
                 PriceIndex + Age + HouseholdSize + Income + Day_of_week + Week + Year,
               data = FullData)
# clean version: excluding time fixed effects from output
modelmatrix <- model.matrix(ols)
keep_main <- colnames(modelmatrix)
rm(modelmatrix)
keep_main <- keep_main[!grepl("Day",keep_main) & !grepl("Week",keep_main) & !grepl("Year",keep_main)]

# save clean output
#OLS_output_clean(ols, "FinalModel_replicateStudy1_simpleOLS_Clean", keep_main)
rm(ols)

```







```{r moderators in TOBIT 2}

heckman_main <- selection(selection = !is.na(Total_volume_sales) ~  PriceIndex + HouseholdInventory +
                              PromoIndex*PromoSensitivity + PromoIndex*PLShare + PromoIndex*BrandLoyalty + 
                              PanicIndex*PromoSensitivity + PanicIndex*PLShare + PanicIndex*BrandLoyalty + 
                              PromoIndex*Average_IPT +
                              PanicIndex*Average_IPT +
                              PromoIndex*Age + PromoIndex*HouseholdSize + PromoIndex*Income +
                              PanicIndex*Age + PanicIndex*HouseholdSize + PanicIndex*Income +
                              Day_of_week + Week + Year,
                            outcome =          Total_volume_sales  ~  PriceIndex + HouseholdInventory +
                              PromoIndex*PromoSensitivity + PromoIndex*PLShare + PromoIndex*BrandLoyalty +
                              PanicIndex*PromoSensitivity + PanicIndex*PLShare + PanicIndex*BrandLoyalty +
                              PromoIndex*Average_Q +
                              PanicIndex*Average_Q +
                              PromoIndex*Age + PromoIndex*HouseholdSize + PromoIndex*Income +
                              PanicIndex*Age + PanicIndex*HouseholdSize + PanicIndex*Income +
                              Day_of_week + Week + Year,
                          data = FullData, method ="2step")

modelsummary(heckman_main, shape = term ~ component + statistic, stars = T, statistic = c("std.error", "p.value"))


```


